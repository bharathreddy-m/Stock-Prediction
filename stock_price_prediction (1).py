# -*- coding: utf-8 -*-
"""Stock price prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UdzLmSeYqMsv-Wx2FUkB2Z0v5Ye0bPgF

**Stock Price Prediction - Yahoo Dataset - Apple (AAPL) stock data**
"""

#import all the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#import model
df = pd.read_csv('/content/AAPL_data.csv')

df

#see columns
df.columns

# summary of the data
df.describe()

# see datatypes
df.info()

"""Since 2 columns are currently object (string) types, you need to convert them into numeric types (float or int), so that Machine Learning models can process them."""

# convert date column from object to numeric
# Convert date to Datetime Format
# The date column should be in datetime format for time series analysis.
df['date'] = pd.to_datetime(df['date'])

# see unique values in Name column
df['Name'].unique()

"""Since, all the name column contains AAPL. It doesn't help the model. Best is to drop the column."""

df = df.drop('Name', axis=1)
df

df.info()

# Set date as the index (useful for time series analysis):
df.set_index('date', inplace=True)

df

# If you need date as a column again
df = df.reset_index()

df.info()

"""**Defining Target - Since we're predicting the next day's closing price, we need to shift the data.**

* This moves the close price one step up, so each row’s target is the next day’s close price.
* The last row will have NaN in target (because there's no next day), so we should drop it later.
"""

df['target'] = df['close'].shift(-1)

df

df['target'].unique()

# the last row has "nan" because there is no next day. so we have to drop last row
df.dropna(inplace=True)

print(df.tail())

df

"""**Feature Engineering**

Why Do We Need Feature Engineering?
Feature engineering is essential in machine learning for several reasons:
1. Improve Model Performance - Creating relevant features can lead to better predictions. For example, using moving averages can help capture trends in stock prices.

2. Enhance Interpretability - Adding features that make sense for your domain (like price changes) can help you understand your model's predictions better.

3. Utilize Available Data - You can transform existing data into new features that may provide additional insights, helping the model learn more effectively.

4. Prepare for Time-Series Analysis - In time series, lagged features (previous days’ prices) are crucial as they provide context for the target variable.

**Moving averages - Calculate moving averages to smooth out price fluctuations and identify trends.**
"""

# Capture short-term trends, useful for active traders.
df['SMA_5'] = df['close'].rolling(window=5).mean()  # 5-day moving average
df['SMA_20'] = df['close'].rolling(window=20).mean()  # 20-day moving average

df['SMA_5'].unique()

df['SMA_20'].unique()

# Provide a bigger picture, useful for long-term investors.
df['SMA_50'] = df['close'].rolling(window=50).mean() # 50-day moving average
df['SMA_200'] = df['close'].rolling(window=200).mean() #200-day moving average

df['SMA_50'].unique()

df['SMA_200'].unique()

"""**Price Changes - Create features to capture daily price changes, which can help the model learn about volatility.**"""

df['price_change'] = df['close'].diff() #Difference from the previous day's close
df['pct_change'] = df['close'].pct_change() #percentage change

df['price_change'].unique()

# looking for nan
nan_count = df['price_change'].isna().sum()
print(nan_count)

# looking for -8.2150e-01
val = (df['price_change']==-0.85140).sum()
print(val)

#allowing floating point precision
val = (df['price_change'].sub(-0.85140).abs() < 1e-5).sum()
print(val)

"""**Lagged Features - Use previous days' prices as features to predict the next day’s price.**"""

df['Lag1'] = df['close'].shift(1) #previous day closing
df['Lag2'] = df['close'].shift(2) #previous 2 day closing

df

"""**Volume Features - You can create features based on volume, such as the average trading volume over the last few days:**"""

df['volume_5'] = df['volume'].rolling(window=5).mean()

df.head(5)

df.columns

"""**Data Visualization**"""

#put the date column as index
df.set_index('date', inplace=True)

df.head(5)

"""Price Trends Over Time - Since this is a time series dataset, start by plotting the close price over time."""

plt.plot(df.index, df['close'], label='Closing Price', color='green')
plt.xlabel('Year')
plt.ylabel('Closing price (USD)')
plt.title('Apple closing price trend')
plt.legend()

"""Moving Averages - A moving average smooths out short-term fluctuations and highlights longer-term trends."""

plt.plot(df.index, df['close'], label="Closing Price", color='blue', alpha=0.6)
plt.plot(df.index, df['SMA_5'], label="5-Day MA", color='red', linestyle='dashed')
plt.plot(df.index, df['SMA_20'], label="20-Day MA", color='green', linestyle='dashed')

plt.xlabel("Date")
plt.ylabel("Stock Price (USD)")
plt.title("Apple Stock Price with 5 & 20-Day Moving Averages")
plt.legend()
plt.show()

plt.plot(df.index, df['close'], label="Closing Price", color='blue', alpha=0.6)
plt.plot(df.index, df['SMA_50'], label="50-Day MA", color='red', linestyle='dashed')
plt.plot(df.index, df['SMA_200'], label="200-Day MA", color='green', linestyle='dashed')

plt.xlabel("Date")
plt.ylabel("Stock Price (USD)")
plt.title("Apple Stock Price with 50 & 200-Day Moving Averages")
plt.legend()
plt.show()

"""**Volume Analysis - Stock price movement along with trading volume can indicate strong or weak trends.**"""

fig, ax1 = plt.subplots(figsize=(12,6))

ax1.set_xlabel("Date")
ax1.set_ylabel("Closing Price (USD)", color='blue')
ax1.plot(df.index, df['close'], label="Closing Price", color='blue', alpha=0.6)
ax1.tick_params(axis='y', labelcolor='blue')

ax2 = ax1.twinx()  # Create a second y-axis
ax2.set_ylabel("Volume", color='red')
ax2.bar(df.index, df['volume'], label="Trading Volume", color='red', alpha=0.3)
ax2.tick_params(axis='y', labelcolor='red')

fig.suptitle("Apple Stock Price and Trading Volume")
fig.legend(loc="upper left")
plt.show()

"""**Correlation Matrix (Feature Relationships) - We analyze relationships between different stock price attributes.**"""

sns.heatmap(df[['open', 'high', 'low', 'close', 'volume']].corr(), annot=True, cmap="coolwarm",linewidths=0.5)

"""**Feature Scaling - Feature scaling is important in your stock price prediction project because**

1. Brings All Features to a Similar Scale
Your dataset has different ranges for different features:
* Stock prices (open, high, low, close) → Typically range in hundreds (e.g., $150, $200).
* Volume → Can be in millions (e.g., 10,000,000).
* Moving Averages (50-day, 200-day) → Again, have different scales.
  
  If we don't scale, models like Gradient Descent, KNN, and SVM will struggle because they treat larger values as more important.

2. Helps Gradient Descent Converge Faster
Many machine learning models (like Linear Regression, SVR, Neural Networks) use gradient descent.
* Without scaling: Large values lead to unstable updates, causing slow or non-converging models.
* With scaling: The optimization process is smoother, and the model learns efficiently.

3. Needed for Distance-Based Models
Models like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Decision Trees calculate distances between points.
* If features are on different scales, distance calculations will be biased toward larger-valued features (like volume).
* Scaling ensures all features contribute equally.

Since this is a regression task, we usually use:

MinMaxScaler (Scales between 0 and 1)

StandardScaler (Scales to mean 0, variance 1)

Check how data is distributed
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,5))
sns.histplot(df['close'], bins=30, kde=True)
plt.title("Histogram of Closing Prices")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Create histograms for all numerical features
num_features = df.select_dtypes(include=['float64', 'int64']).shape[1]  # Count numerical features
nrows = (num_features // 3) + (num_features % 3 > 0)  # Calculate number of rows needed

df.hist(bins=30, layout=(nrows, 3), edgecolor='black')
plt.suptitle("Histograms of All Features")
plt.show()

# removes positively skewed data
df['volume'] = np.log(df['volume'] + 1)  # Add 1 to avoid log(0)

skewness = df.skew()
print(skewness)

sns.histplot(df['volume'])

#put index back as a column
df = df.reset_index()

df

df.drop('volume_log', axis=1)

#Divide dataset into x and y
x = df.drop(['target', 'date'],axis=1)
x.columns

y = df[['target', 'date']]
y.columns

# Do standard scalar
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_scaled = sc.fit_transform(x)

x_df = pd.DataFrame(x_scaled, columns=x.columns)

x_df.head(5)

y.columns

x_df.shape

y.shape

print(x_df.index)
print(y.index)  # This will show the index of y if it is a Series

y = pd.Series(y)  # Convert y to a pandas Series if it's not
y

# concatenate x and y into df
df = pd.concat([x_df, y], axis=1)

df.head()

df['target'].unique()

df.head(5)

"""**Time Series Processing - Resampling, Rolling Statistics, Differencing**

Resampling -

Purpose: Resampling helps to change the frequency of your time series data (e.g., daily to weekly or monthly). This can be useful to analyze trends over a longer period and reduce noise.

How to Apply: You can use the resample method in pandas. For example, to resample your data to monthly frequency.
"""

# put date column as index
df.set_index('date', inplace=True)

df

"""Our data contains "Nan" values and regression can't process those. Deleting and filling with mean, median is not a best idea because it disrupts the flow"""

# Backward Fill (If Forward Fill is Not Enough) If the dataset starts with NaNs (e.g., first SMA_200 values), use backward fill as a backup:
df.bfill(inplace=True)

df

print(df.isnull().sum())

"""Split the data into X and Y"""

x = df.drop('target', axis=1)
x.columns

print(x.dtypes)

y = df['target']
y

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=42, shuffle=False)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

"""Model Building"""

from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures

a = LinearRegression()
b = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())

# Fit all the models at once to see their accuracies
models = [a, b]
model_names = ['Linear Regression', 'Polynomial regression']

#store results
results_1 = {}

#import evaluation metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score

# Do all training, testing at once
for model, names in zip(models, model_names):

  #Fit the model here
  model.fit(x_train, y_train)

  #Make Predictions
  train_pred = model.predict(x_train)
  test_pred = model.predict(x_test)

  #Calculate training error
  train_mse = mean_squared_error(y_train, train_pred)
  train_mae = mean_absolute_error(y_train, train_pred)
  train_rmse = np.sqrt(train_mse)
  train_r2 = r2_score(y_train, train_pred)

  #calculate test error
  test_mse = mean_squared_error(y_test, test_pred)
  test_mae = mean_absolute_error(y_test, test_pred)
  test_rmse = np.sqrt(test_mse)
  test_r2 = r2_score(y_test, test_pred)

  #print results
  print(f' Model -- {names}')
  print(f' Training results -- mse = {train_mse:.4f}, mae = {train_mae:.4f}, rmse = {train_rmse:.4f}, r2 = {train_r2:.4f}')
  print(f' Testing results -- mse = {test_mse:.4f}, mae = {test_mae:.4f}, rmse = {test_rmse:.4f}, r2 = {test_r2:.4f}')
  print("-" * 80)